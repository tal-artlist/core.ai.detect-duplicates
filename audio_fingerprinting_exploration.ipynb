{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Audio Fingerprinting Exploration with PyAcoustID\n",
        "\n",
        "This notebook explores using PyAcoustID for audio fingerprinting and duplicate detection in our Artlist and MotionArray catalogs.\n",
        "\n",
        "## Goals:\n",
        "1. Understand how PyAcoustID works with our audio files\n",
        "2. Generate fingerprints for sample audio files\n",
        "3. Compare fingerprints to detect duplicates\n",
        "4. Experiment with similarity thresholds\n",
        "5. Evaluate performance and accuracy\n",
        "\n",
        "## Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'librosa'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01macoustid\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlibrosa\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'librosa'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import acoustid\n",
        "import librosa\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import time\n",
        "from typing import List, Dict, Tuple\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set up paths\n",
        "artlist_dir = Path(\"downloads/artlist\")\n",
        "motionarray_dir = Path(\"downloads/motionarray\")\n",
        "\n",
        "print(\"Available functions in acoustid:\")\n",
        "print([f for f in dir(acoustid) if not f.startswith('_')])\n",
        "print(f\"\\nArtlist directory exists: {artlist_dir.exists()}\")\n",
        "print(f\"MotionArray directory exists: {motionarray_dir.exists()}\")\n",
        "\n",
        "if artlist_dir.exists():\n",
        "    artlist_files = list(artlist_dir.glob(\"*.mp3\")) + list(artlist_dir.glob(\"*.wav\"))\n",
        "    print(f\"Found {len(artlist_files)} Artlist audio files\")\n",
        "    \n",
        "if motionarray_dir.exists():\n",
        "    motionarray_files = list(motionarray_dir.glob(\"*.mp3\")) + list(motionarray_dir.glob(\"*.wav\"))\n",
        "    print(f\"Found {len(motionarray_files)} MotionArray audio files\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding PyAcoustID\n",
        "\n",
        "PyAcoustID is a Python library for generating acoustic fingerprints and looking up metadata from the AcoustID service. Let's explore its key functions:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test basic functionality\n",
        "print(\"Testing PyAcoustID functionality:\")\n",
        "print(f\"Have chromaprint: {acoustid.have_chromaprint()}\")\n",
        "print(f\"Have audioread: {acoustid.have_audioread()}\")\n",
        "\n",
        "# Key functions we'll use:\n",
        "print(\"\\nKey functions for our use case:\")\n",
        "print(\"1. acoustid.fingerprint_file() - Generate fingerprint from audio file\")\n",
        "print(\"2. acoustid.compare_fingerprints() - Compare two fingerprints for similarity\")\n",
        "print(\"3. acoustid.fingerprint() - Generate fingerprint from raw audio data\")\n",
        "\n",
        "# Let's check if we have any audio files to work with\n",
        "sample_files = []\n",
        "if artlist_dir.exists():\n",
        "    sample_files.extend(list(artlist_dir.glob(\"*.mp3\"))[:3])  # Take first 3 MP3s\n",
        "    sample_files.extend(list(artlist_dir.glob(\"*.wav\"))[:3])  # Take first 3 WAVs\n",
        "\n",
        "if motionarray_dir.exists():\n",
        "    sample_files.extend(list(motionarray_dir.glob(\"*.mp3\"))[:3])  # Take first 3 MP3s\n",
        "    sample_files.extend(list(motionarray_dir.glob(\"*.wav\"))[:3])  # Take first 3 WAVs\n",
        "\n",
        "print(f\"\\nSample files for testing: {len(sample_files)}\")\n",
        "for i, file in enumerate(sample_files[:5]):  # Show first 5\n",
        "    print(f\"{i+1}. {file.name} ({file.stat().st_size / (1024*1024):.1f} MB)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generating Audio Fingerprints\n",
        "\n",
        "Let's generate fingerprints for our sample audio files and see what they look like:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_fingerprint_safe(file_path):\n",
        "    \"\"\"Safely generate fingerprint for an audio file.\"\"\"\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "        duration, fingerprint = acoustid.fingerprint_file(str(file_path))\n",
        "        processing_time = time.time() - start_time\n",
        "        \n",
        "        return {\n",
        "            'file': file_path.name,\n",
        "            'duration': duration,\n",
        "            'fingerprint': fingerprint,\n",
        "            'processing_time': processing_time,\n",
        "            'fingerprint_length': len(fingerprint) if fingerprint else 0,\n",
        "            'success': True,\n",
        "            'error': None\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            'file': file_path.name,\n",
        "            'duration': None,\n",
        "            'fingerprint': None,\n",
        "            'processing_time': None,\n",
        "            'fingerprint_length': 0,\n",
        "            'success': False,\n",
        "            'error': str(e)\n",
        "        }\n",
        "\n",
        "# Generate fingerprints for sample files\n",
        "print(\"Generating fingerprints for sample files...\")\n",
        "fingerprint_results = []\n",
        "\n",
        "for i, file_path in enumerate(sample_files[:5]):  # Test with first 5 files\n",
        "    print(f\"\\nProcessing {i+1}/5: {file_path.name}\")\n",
        "    result = generate_fingerprint_safe(file_path)\n",
        "    fingerprint_results.append(result)\n",
        "    \n",
        "    if result['success']:\n",
        "        print(f\"  ‚úì Duration: {result['duration']:.1f}s\")\n",
        "        print(f\"  ‚úì Fingerprint length: {result['fingerprint_length']} characters\")\n",
        "        print(f\"  ‚úì Processing time: {result['processing_time']:.2f}s\")\n",
        "        print(f\"  ‚úì Fingerprint preview: {result['fingerprint'][:50]}...\")\n",
        "    else:\n",
        "        print(f\"  ‚úó Error: {result['error']}\")\n",
        "\n",
        "# Create a summary DataFrame\n",
        "df_fingerprints = pd.DataFrame(fingerprint_results)\n",
        "print(f\"\\nSummary:\")\n",
        "print(f\"Successful fingerprints: {df_fingerprints['success'].sum()}/{len(df_fingerprints)}\")\n",
        "print(f\"Average processing time: {df_fingerprints[df_fingerprints['success']]['processing_time'].mean():.2f}s\")\n",
        "print(f\"Average fingerprint length: {df_fingerprints[df_fingerprints['success']]['fingerprint_length'].mean():.0f} chars\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparing Fingerprints for Similarity\n",
        "\n",
        "Now let's test the fingerprint comparison functionality to understand how similarity detection works:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get successful fingerprints for comparison\n",
        "successful_results = [r for r in fingerprint_results if r['success']]\n",
        "print(f\"We have {len(successful_results)} successful fingerprints to compare\")\n",
        "\n",
        "if len(successful_results) >= 2:\n",
        "    print(\"\\nTesting fingerprint comparison...\")\n",
        "    \n",
        "    # Compare each fingerprint with every other fingerprint\n",
        "    comparison_results = []\n",
        "    \n",
        "    for i in range(len(successful_results)):\n",
        "        for j in range(i + 1, len(successful_results)):\n",
        "            file1 = successful_results[i]\n",
        "            file2 = successful_results[j]\n",
        "            \n",
        "            try:\n",
        "                # Compare fingerprints\n",
        "                score = acoustid.compare_fingerprints(file1['fingerprint'], file2['fingerprint'])\n",
        "                \n",
        "                comparison_results.append({\n",
        "                    'file1': file1['file'],\n",
        "                    'file2': file2['file'],\n",
        "                    'similarity_score': score,\n",
        "                    'duration1': file1['duration'],\n",
        "                    'duration2': file2['duration'],\n",
        "                    'duration_diff': abs(file1['duration'] - file2['duration'])\n",
        "                })\n",
        "                \n",
        "                print(f\"  {file1['file'][:30]:<30} vs {file2['file'][:30]:<30} = {score:.4f}\")\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"  Error comparing {file1['file']} vs {file2['file']}: {e}\")\n",
        "    \n",
        "    # Analyze comparison results\n",
        "    if comparison_results:\n",
        "        df_comparisons = pd.DataFrame(comparison_results)\n",
        "        \n",
        "        print(f\"\\nComparison Statistics:\")\n",
        "        print(f\"Total comparisons: {len(df_comparisons)}\")\n",
        "        print(f\"Average similarity: {df_comparisons['similarity_score'].mean():.4f}\")\n",
        "        print(f\"Max similarity: {df_comparisons['similarity_score'].max():.4f}\")\n",
        "        print(f\"Min similarity: {df_comparisons['similarity_score'].min():.4f}\")\n",
        "        print(f\"Std deviation: {df_comparisons['similarity_score'].std():.4f}\")\n",
        "        \n",
        "        # Show most similar pairs\n",
        "        print(f\"\\nMost similar pairs:\")\n",
        "        top_similar = df_comparisons.nlargest(3, 'similarity_score')\n",
        "        for _, row in top_similar.iterrows():\n",
        "            print(f\"  {row['similarity_score']:.4f}: {row['file1']} vs {row['file2']}\")\n",
        "            \n",
        "        # Show least similar pairs\n",
        "        print(f\"\\nLeast similar pairs:\")\n",
        "        least_similar = df_comparisons.nsmallest(3, 'similarity_score')\n",
        "        for _, row in least_similar.iterrows():\n",
        "            print(f\"  {row['similarity_score']:.4f}: {row['file1']} vs {row['file2']}\")\n",
        "            \n",
        "else:\n",
        "    print(\"Not enough successful fingerprints to perform comparisons\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizing Similarity Scores\n",
        "\n",
        "Let's create some visualizations to better understand the similarity score distribution:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'df_comparisons' in locals() and len(df_comparisons) > 0:\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    \n",
        "    # Histogram of similarity scores\n",
        "    axes[0, 0].hist(df_comparisons['similarity_score'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "    axes[0, 0].set_title('Distribution of Similarity Scores')\n",
        "    axes[0, 0].set_xlabel('Similarity Score')\n",
        "    axes[0, 0].set_ylabel('Frequency')\n",
        "    axes[0, 0].axvline(df_comparisons['similarity_score'].mean(), color='red', linestyle='--', label='Mean')\n",
        "    axes[0, 0].legend()\n",
        "    \n",
        "    # Box plot of similarity scores\n",
        "    axes[0, 1].boxplot(df_comparisons['similarity_score'])\n",
        "    axes[0, 1].set_title('Similarity Score Box Plot')\n",
        "    axes[0, 1].set_ylabel('Similarity Score')\n",
        "    \n",
        "    # Scatter plot: Duration difference vs Similarity\n",
        "    axes[1, 0].scatter(df_comparisons['duration_diff'], df_comparisons['similarity_score'], alpha=0.6)\n",
        "    axes[1, 0].set_title('Duration Difference vs Similarity Score')\n",
        "    axes[1, 0].set_xlabel('Duration Difference (seconds)')\n",
        "    axes[1, 0].set_ylabel('Similarity Score')\n",
        "    \n",
        "    # Similarity score vs comparison index (to see patterns)\n",
        "    axes[1, 1].plot(range(len(df_comparisons)), sorted(df_comparisons['similarity_score'], reverse=True), 'o-')\n",
        "    axes[1, 1].set_title('Similarity Scores (Sorted)')\n",
        "    axes[1, 1].set_xlabel('Comparison Index')\n",
        "    axes[1, 1].set_ylabel('Similarity Score')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print some insights\n",
        "    print(\"Insights from the analysis:\")\n",
        "    print(f\"1. Score range: {df_comparisons['similarity_score'].min():.4f} to {df_comparisons['similarity_score'].max():.4f}\")\n",
        "    print(f\"2. Most comparisons have scores around {df_comparisons['similarity_score'].median():.4f} (median)\")\n",
        "    \n",
        "    # Suggest potential thresholds\n",
        "    q75 = df_comparisons['similarity_score'].quantile(0.75)\n",
        "    q90 = df_comparisons['similarity_score'].quantile(0.90)\n",
        "    q95 = df_comparisons['similarity_score'].quantile(0.95)\n",
        "    \n",
        "    print(f\"3. Potential similarity thresholds:\")\n",
        "    print(f\"   - Conservative (top 25%): {q75:.4f}\")\n",
        "    print(f\"   - Moderate (top 10%): {q90:.4f}\")\n",
        "    print(f\"   - Strict (top 5%): {q95:.4f}\")\n",
        "    \n",
        "else:\n",
        "    print(\"No comparison data available for visualization\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing Different Audio Formats\n",
        "\n",
        "Let's see how PyAcoustID handles different audio formats (MP3 vs WAV) and if there are any differences:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze by file format\n",
        "if len(successful_results) > 0:\n",
        "    format_analysis = {'MP3': [], 'WAV': []}\n",
        "    \n",
        "    for result in successful_results:\n",
        "        if result['file'].lower().endswith('.mp3'):\n",
        "            format_analysis['MP3'].append(result)\n",
        "        elif result['file'].lower().endswith('.wav'):\n",
        "            format_analysis['WAV'].append(result)\n",
        "    \n",
        "    print(\"Format Analysis:\")\n",
        "    for format_type, files in format_analysis.items():\n",
        "        if files:\n",
        "            avg_duration = np.mean([f['duration'] for f in files])\n",
        "            avg_processing_time = np.mean([f['processing_time'] for f in files])\n",
        "            avg_fingerprint_length = np.mean([f['fingerprint_length'] for f in files])\n",
        "            \n",
        "            print(f\"\\n{format_type} files ({len(files)} files):\")\n",
        "            print(f\"  Average duration: {avg_duration:.1f}s\")\n",
        "            print(f\"  Average processing time: {avg_processing_time:.2f}s\")\n",
        "            print(f\"  Average fingerprint length: {avg_fingerprint_length:.0f} chars\")\n",
        "            print(f\"  Processing speed: {avg_duration/avg_processing_time:.1f}x realtime\")\n",
        "    \n",
        "    # Compare MP3 vs WAV if we have both\n",
        "    if format_analysis['MP3'] and format_analysis['WAV']:\n",
        "        print(f\"\\nCross-format comparison (MP3 vs WAV):\")\n",
        "        cross_format_scores = []\n",
        "        \n",
        "        for mp3_file in format_analysis['MP3'][:2]:  # Limit to avoid too many comparisons\n",
        "            for wav_file in format_analysis['WAV'][:2]:\n",
        "                try:\n",
        "                    score = acoustid.compare_fingerprints(mp3_file['fingerprint'], wav_file['fingerprint'])\n",
        "                    cross_format_scores.append(score)\n",
        "                    print(f\"  {mp3_file['file'][:25]:<25} vs {wav_file['file'][:25]:<25} = {score:.4f}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"  Error comparing {mp3_file['file']} vs {wav_file['file']}: {e}\")\n",
        "        \n",
        "        if cross_format_scores:\n",
        "            print(f\"  Average cross-format similarity: {np.mean(cross_format_scores):.4f}\")\n",
        "else:\n",
        "    print(\"No successful results to analyze by format\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Performance Analysis and Scalability\n",
        "\n",
        "Let's analyze the performance characteristics for scaling to our full catalog:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Performance analysis\n",
        "if len(successful_results) > 0:\n",
        "    total_audio_duration = sum([r['duration'] for r in successful_results])\n",
        "    total_processing_time = sum([r['processing_time'] for r in successful_results])\n",
        "    \n",
        "    print(\"Performance Analysis:\")\n",
        "    print(f\"Total audio analyzed: {total_audio_duration:.1f} seconds ({total_audio_duration/60:.1f} minutes)\")\n",
        "    print(f\"Total processing time: {total_processing_time:.1f} seconds ({total_processing_time/60:.1f} minutes)\")\n",
        "    print(f\"Processing speed: {total_audio_duration/total_processing_time:.1f}x realtime\")\n",
        "    \n",
        "    # Estimate for full catalog\n",
        "    print(f\"\\nScaling estimates:\")\n",
        "    \n",
        "    # Assume we have 100 files from each catalog (200 total)\n",
        "    estimated_files = 200\n",
        "    avg_duration = total_audio_duration / len(successful_results)\n",
        "    avg_processing_time = total_processing_time / len(successful_results)\n",
        "    \n",
        "    estimated_total_duration = estimated_files * avg_duration\n",
        "    estimated_processing_time = estimated_files * avg_processing_time\n",
        "    \n",
        "    print(f\"For {estimated_files} files:\")\n",
        "    print(f\"  Estimated fingerprinting time: {estimated_processing_time/60:.1f} minutes\")\n",
        "    print(f\"  Estimated total audio: {estimated_total_duration/3600:.1f} hours\")\n",
        "    \n",
        "    # Comparison complexity\n",
        "    total_comparisons = (estimated_files * (estimated_files - 1)) // 2\n",
        "    print(f\"  Total pairwise comparisons needed: {total_comparisons:,}\")\n",
        "    \n",
        "    # Estimate comparison time (assume 0.001 seconds per comparison)\n",
        "    comparison_time_per_pair = 0.001  # seconds\n",
        "    total_comparison_time = total_comparisons * comparison_time_per_pair\n",
        "    \n",
        "    print(f\"  Estimated comparison time: {total_comparison_time/60:.1f} minutes\")\n",
        "    print(f\"  Total processing time: {(estimated_processing_time + total_comparison_time)/60:.1f} minutes\")\n",
        "    \n",
        "    # Memory requirements\n",
        "    avg_fingerprint_size = np.mean([len(r['fingerprint']) for r in successful_results])\n",
        "    total_fingerprint_memory = estimated_files * avg_fingerprint_size\n",
        "    \n",
        "    print(f\"\\nMemory estimates:\")\n",
        "    print(f\"  Average fingerprint size: {avg_fingerprint_size:.0f} characters\")\n",
        "    print(f\"  Total fingerprint storage: {total_fingerprint_memory/1024:.1f} KB\")\n",
        "    \n",
        "else:\n",
        "    print(\"No successful results for performance analysis\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusions and Recommendations\n",
        "\n",
        "Based on our exploration, here are the key findings and recommendations for implementing duplicate detection:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== CONCLUSIONS AND RECOMMENDATIONS ===\")\n",
        "print()\n",
        "\n",
        "if len(successful_results) > 0:\n",
        "    print(\"‚úÖ PyAcoustID Successfully Working:\")\n",
        "    print(f\"   - Successfully processed {len(successful_results)} audio files\")\n",
        "    print(f\"   - Average processing speed: {total_audio_duration/total_processing_time:.1f}x realtime\")\n",
        "    print(f\"   - Works with both MP3 and WAV formats\")\n",
        "    print()\n",
        "    \n",
        "    if 'df_comparisons' in locals() and len(df_comparisons) > 0:\n",
        "        print(\"üìä Similarity Analysis:\")\n",
        "        print(f\"   - Similarity scores range from {df_comparisons['similarity_score'].min():.4f} to {df_comparisons['similarity_score'].max():.4f}\")\n",
        "        print(f\"   - Average similarity: {df_comparisons['similarity_score'].mean():.4f}\")\n",
        "        print(f\"   - Recommended thresholds:\")\n",
        "        print(f\"     * High confidence duplicates: > {df_comparisons['similarity_score'].quantile(0.95):.4f}\")\n",
        "        print(f\"     * Potential duplicates: > {df_comparisons['similarity_score'].quantile(0.90):.4f}\")\n",
        "        print(f\"     * Similar tracks: > {df_comparisons['similarity_score'].quantile(0.75):.4f}\")\n",
        "        print()\n",
        "    \n",
        "    print(\"üöÄ Implementation Strategy:\")\n",
        "    print(\"   1. Batch process all 200 files to generate fingerprints\")\n",
        "    print(\"   2. Store fingerprints in database with metadata\")\n",
        "    print(\"   3. Implement efficient comparison algorithm\")\n",
        "    print(\"   4. Use similarity thresholds to classify matches\")\n",
        "    print(\"   5. Manual review for borderline cases\")\n",
        "    print()\n",
        "    \n",
        "    print(\"‚ö° Performance Considerations:\")\n",
        "    print(f\"   - Fingerprinting: ~{estimated_processing_time/60:.0f} minutes for 200 files\")\n",
        "    print(f\"   - Comparisons: ~{total_comparison_time/60:.0f} minutes for all pairs\")\n",
        "    print(f\"   - Total time: ~{(estimated_processing_time + total_comparison_time)/60:.0f} minutes\")\n",
        "    print(\"   - Memory usage: Very low (fingerprints are compact)\")\n",
        "    print()\n",
        "    \n",
        "    print(\"üîç Next Steps:\")\n",
        "    print(\"   1. Create duplicate detection script\")\n",
        "    print(\"   2. Process full catalog (100 files each)\")\n",
        "    print(\"   3. Implement database storage for fingerprints\")\n",
        "    print(\"   4. Build comparison and reporting system\")\n",
        "    print(\"   5. Add manual review interface\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå Issues Found:\")\n",
        "    print(\"   - Could not process audio files successfully\")\n",
        "    print(\"   - Check audio file formats and pyacoustid installation\")\n",
        "    print(\"   - Verify chromaprint binary is available\")\n",
        "\n",
        "print()\n",
        "print(\"=== END OF ANALYSIS ===\")\n",
        "\n",
        "# Save results for later use\n",
        "if len(successful_results) > 0:\n",
        "    # Create a summary dictionary\n",
        "    analysis_summary = {\n",
        "        'successful_files': len(successful_results),\n",
        "        'total_files_tested': len(fingerprint_results),\n",
        "        'average_processing_time': total_processing_time / len(successful_results),\n",
        "        'average_duration': total_audio_duration / len(successful_results),\n",
        "        'processing_speed_ratio': total_audio_duration / total_processing_time,\n",
        "    }\n",
        "    \n",
        "    if 'df_comparisons' in locals() and len(df_comparisons) > 0:\n",
        "        analysis_summary.update({\n",
        "            'total_comparisons': len(df_comparisons),\n",
        "            'avg_similarity': df_comparisons['similarity_score'].mean(),\n",
        "            'max_similarity': df_comparisons['similarity_score'].max(),\n",
        "            'min_similarity': df_comparisons['similarity_score'].min(),\n",
        "            'threshold_95': df_comparisons['similarity_score'].quantile(0.95),\n",
        "            'threshold_90': df_comparisons['similarity_score'].quantile(0.90),\n",
        "            'threshold_75': df_comparisons['similarity_score'].quantile(0.75),\n",
        "        })\n",
        "    \n",
        "    print(f\"\\nüìÅ Analysis summary saved to 'analysis_summary' variable\")\n",
        "    print(\"   Use this data to inform the duplicate detection script implementation\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
